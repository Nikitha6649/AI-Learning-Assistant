
// This is an autogenerated file from Firebase Studio.
'use server';
/**
 * @fileOverview A multimodal AI agent that generates contextual responses including text and relevant visual aids based on student input.
 *
 * - generateMultimodalResponse - A function that handles the generation of multimodal responses.
 * - GenerateMultimodalResponseInput - The input type for the generateMultimodalResponse function.
 * - GenerateMultimodalResponseOutput - The return type for the generateMultimodalResponse function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const GenerateMultimodalResponseInputSchema = z.object({
  queryText: z.string().optional().describe('The text query from the student.'),
  queryVoiceDataUri: z
    .string()
    .optional()
    .describe(
      "The voice query from the student as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  queryImageDataUri: z
    .string()
    .optional()
    .describe(
      "The image query from the student as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
});
export type GenerateMultimodalResponseInput = z.infer<typeof GenerateMultimodalResponseInputSchema>;

const GenerateMultimodalResponseOutputSchema = z.object({
  textResponse: z.string().describe('The textual explanation.'),
  chartDataUri: z
    .string()
    .optional()
    .describe(
      "The chart or diagram data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
});
export type GenerateMultimodalResponseOutput = z.infer<typeof GenerateMultimodalResponseOutputSchema>;

export async function generateMultimodalResponse(input: GenerateMultimodalResponseInput): Promise<GenerateMultimodalResponseOutput> {
  return generateMultimodalResponseFlow(input);
}

// Define the schema for the simplified text-generation prompt's input
const SimplifiedPromptInputSchema = z.object({
  queryText: z.string().optional(),
  queryVoiceDataUri: z.string().optional(),
  queryImageDataUri: z.string().optional(),
  generatedChartDataUri: z.string().optional().describe("A pre-generated chart/diagram related to the query, if available."),
});

// Define the schema for the simplified text-generation prompt's output
const TextOnlyOutputSchema = z.object({
  textResponse: z.string().describe('The textual explanation.'),
});

// Define the simplified prompt for generating only the text response
const simplifiedTextPrompt = ai.definePrompt({
  name: 'simplifiedTextPrompt',
  input: { schema: SimplifiedPromptInputSchema },
  output: { schema: TextOnlyOutputSchema },
  prompt: `You are an AI-powered interactive learning assistant for classrooms.
A student has provided the following input:
{{#if queryText}}
Text Query: {{{queryText}}}
{{/if}}
{{#if queryImageDataUri}}
Image Query: {{media url=queryImageDataUri}}
{{/if}}
{{#if queryVoiceDataUri}}
Voice Query: {{media url=queryVoiceDataUri}} (Note: Audio transcription is not yet available. Please respond based on other provided inputs or general knowledge if this is the only input.)
{{/if}}

{{#if generatedChartDataUri}}
A relevant visual aid (chart or diagram) has been generated for you:
Visual Aid: {{media url=generatedChartDataUri}}
Please provide a comprehensive textual explanation that addresses the student's query. If the query and visual aid are related, explain the visual aid in context of the query.
{{else}}
Please provide a comprehensive textual explanation to address the student's query.
{{/if}}

Focus on generating a clear, helpful, and informative textual response.
If multiple inputs are provided (e.g., text and image), synthesize them in your explanation.
`,
});


const generateMultimodalResponseFlow = ai.defineFlow(
  {
    name: 'generateMultimodalResponseFlow',
    inputSchema: GenerateMultimodalResponseInputSchema,
    outputSchema: GenerateMultimodalResponseOutputSchema,
  },
  async (input: GenerateMultimodalResponseInput): Promise<GenerateMultimodalResponseOutput> => {
    let generatedChartDataUri: string | undefined = undefined;
    let shouldAttemptChartGeneration = false;

    // Heuristic to decide if a chart should be generated
    if (input.queryImageDataUri) {
      // If an image is provided, we might want to generate a supporting chart/diagram.
      shouldAttemptChartGeneration = true;
    } else if (input.queryText) {
      const lowerQueryText = input.queryText.toLowerCase();
      if (lowerQueryText.includes("chart") || lowerQueryText.includes("diagram") || lowerQueryText.includes("visualize") || lowerQueryText.includes("graph")) {
        shouldAttemptChartGeneration = true;
      }
    }

    if (shouldAttemptChartGeneration) {
      const imageGenPromptElements: ({ text: string } | { media: { url: string } })[] = [];
      let basePromptText = "Generate an explanatory chart, graph, or diagram";

      if (input.queryText) {
        basePromptText += ` related to: "${input.queryText}"`;
      } else if (input.queryImageDataUri) {
        basePromptText += ` based on the provided image.`;
      } else {
        basePromptText += ".";
      }
      imageGenPromptElements.push({ text: basePromptText });

      if (input.queryImageDataUri) {
        // Add the user's image to the prompt context if available
        imageGenPromptElements.push({ media: { url: input.queryImageDataUri } });
      }
      
      try {
        const { media } = await ai.generate({
          model: 'googleai/gemini-2.0-flash-exp', // Model capable of image generation
          prompt: imageGenPromptElements,
          config: {
            responseModalities: ['TEXT', 'IMAGE'], // Important: Must specify IMAGE modality
          },
        });
        if (media && media.url) {
          generatedChartDataUri = media.url;
        }
      } catch (error) {
        console.error('Chart generation failed:', error);
        // Proceed without a generated chart
      }
    }

    // Prepare input for the text generation prompt
    const textPromptInput: z.infer<typeof SimplifiedPromptInputSchema> = {
      queryText: input.queryText,
      queryVoiceDataUri: input.queryVoiceDataUri,
      queryImageDataUri: input.queryImageDataUri, // Pass original image for context
      generatedChartDataUri: generatedChartDataUri, // Pass newly generated chart
    };

    const { output: textOutput } = await simplifiedTextPrompt(textPromptInput);

    if (!textOutput || typeof textOutput.textResponse !== 'string') {
      console.error('Failed to get a valid text response from simplifiedTextPrompt.');
      throw new Error('AI failed to generate a textual explanation.');
    }

    return {
      textResponse: textOutput.textResponse,
      chartDataUri: generatedChartDataUri, // This will be the one generated by the flow, if any
    };
  }
);
